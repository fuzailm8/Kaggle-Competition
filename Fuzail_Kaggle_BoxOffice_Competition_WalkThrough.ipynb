{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hi! This is a walk-through of my approach to the Kaggle competition 'TMDB Box Office Prediction' where the objective is to predict the worldwide box-office revenue of a given movie using data from the ['The Movie DataBase'](http://www.themoviedb.org/). \n![image.png](attachment:image.png)\n\n**Motivation** - I found this particular competition highly motivating since it's a question that goes on in every production studio head honcho's mind while pumping millions into making a movie. \"*Will this movie work?*\" It's an age old question across film industries throughout the world and one that no one really has the answer to. Be it USA, India, China, France or South Korea - the dilemma is universal. You'd probably imagine getting in an A-list actor, a multi-million dollar budget, a flooded star cast or a popular genre would lead to the film undoubtedly be a success. \nBut what about the gender ratio of the crew? Or the existence of the movie homepage on the internet? Could these be factors in possibly predicting the money a movie makes? So let's hope we uncover some interesting insights and dive right into it!\nI will try to explain each of my steps as we go along and why I'm doing what I'm doing as best I can. \n\n\nLet's get started by importing some typical libraries which might be useful as we work towards a solution. ","attachments":{"image.png":{"image/png":"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"}}},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport warnings\nimport os \nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We load the training data stored in train.csv into a Pandas dataframe. And let's get a sense of what the data looks like by printing the first 5 rows.   "},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir('../input/tmdb-box-office-prediction/')\ntrain = pd.read_csv('train.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see above, we have 23 columns - 22 input features and 1 output feature - the revenue. The following is a brief description of each feature. It helps to go read and re-read this again and really know what you're dealing with and develop some intuition about how to deal with the data. What features might be crucial, how we can translate some features into ones and if we could combine some features to eliminate redundancy. \n\n**List** - \n\n**id** - Integer unique id of each movie\n\n**belongs_to_collection** - Contains the TMDB Id, Name, Movie Poster and Backdrop URL  of a movie in JSON format. You can see the Poster and Backdrop Image like this: https://image.tmdb.org/t/p/original/<Poster_path_here>. Example: https://image.tmdb.org/t/p/original//iEhb00TGPucF0b4joM1ieyY026U.jpg\n\n**budget**:Budget of a movie in dollars. 0 values mean unknown. \n\n**genres** : Contains all the Genres Name & TMDB Id in JSON Format\n\n**homepage** - Contains the official homepage URL of a movie. Example: http://sonyclassics.com/whiplash/\t, this is the homepage of Whiplash movie.\n\n**imdb_id** - IMDB id of a movie (string). You can visit the IMDB Page like this: https://www.imdb.com/title/<imdb_id_here>\n\n**original_language** - Two digit code of the original language, in which the movie was made. Like: en = English, fr = french. \n\n**original_title** - The original title of a movie. Title & Original title may differ, if the original title is not in English. \n\n**overview** - Brief description of the movie.\n\n**popularity** -  Popularity of the movie in float. \n\n**poster_path** - Poster path of a movie. You can see the full image like this: https://image.tmdb.org/t/p/original/<Poster_path_here>\n\n**production_companies** - All production company name and TMDB id in JSON format of a movie.\n\n**production_countries** - Two digit code and full name of the production company in JSON format.\n\n**release_date** - Release date of a movie in mm/dd/yy format.\n\n**runtime** - Total runtime of a movie in minutes (Integer).\n\n**spoken_languages** - Two digit code and full name of the spoken language. \n\n**status** - Is the movie released or rumored? \n\n**tagline** - Tagline of a movie \n\n**title** - English title of a movie\n\n**Keywords** - TMDB Id and name of all the keywords in JSON format. \n\n**cast** - All cast TMDB id, name, character name, gender (1 = Female, 2 = Male) in JSON format\n\n**crew** - Name, TMDB id, profile path of various kind of crew members job like Director, Writer, Art, Sound etc. \n\n**revenue** - Total revenue earned by a movie in dollars. "},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('./test.csv')\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, the test dataframe is similar to the train dataframe except that it has one column less, which is the revenue column which in turn is what we're supposed to predict after training the model with our training data. "},{"metadata":{},"cell_type":"markdown","source":"To further get to know your data-set, it's always a great idea to use info() so you know the number of missing values and the dtype of each of your features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.info())\nprint(test.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another standard step of your Exploratory Data Analysis would be to use the describe() function which is an essential feature of the Pandas library giving us important statistical information about our features. This gives us a bird's eye view of our data and a quantitatively better idea using the central measures of dispersion. This would mainly only apply to numerical data though. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For example, at first glance, on seeing the 'budget' column, we see that the mean budget is around 22.5 million whereas the median is around 8 million and the mean is actually closer to the 75 percentile value (3rd quartile) (around 29 million). So you can see how the mean is skewed by outlier data points in the budget column. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we begin with our analysis, we will have to combine our training and test dataframe into one. This is being done to smoothly create new features during feature engineering where the same features will have to be created for both the train and the test set. \nWhat we will need to be careful with though is to prevent information leakage from the test set. Remember, the model is to be trained SOLELY on the the training data and nothing else. No information from the test set can be used while building new features. All the processing will be done using the training data only. While creating new features however, the new feature will be created for both the training and test set to ensure they have the same dimensions. What we can do is add create feature for train set and then mimic the same code for test set after that. I prefer to do the operation on our combined dataset so the same feature gets created for both.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"revenue = train['revenue']\ntrain.drop(columns = ['revenue'], axis=1, inplace = True)\n\ncombined = train.append(test)\ncombined.reset_index(inplace = True, drop = True)\n\ncombined","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Perfect! We have created a combined dataset of both the train and test set. The number of rows are the sum of the train and test set. Notice how we have 22 columns. That's because we dropped the revenue column from the train set but we've stored it in the variable 'revenue' for later use. "},{"metadata":{},"cell_type":"markdown","source":"**BUDGET**\n\nNow that we have a sense of what our data looks like, it would be a good time to visualize the relation between features which we think are correlated.\n\nThe most obvious choice that came to mind was seeing the relation between budget and revenue as you'd expect there would be a direct relation between budget and revenue. For this, we use the seaborn library. "},{"metadata":{},"cell_type":"markdown","source":"But before that, let's look a little deeper into the distribution of the revenue variable. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.distplot(revenue)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the revenue curve looks similar to an exponential distribution with a large majority of the data concentrated for revenues less than 200 million. \n\nLet's now look at the relation between the film budget and revenue. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,10))\nsns.scatterplot(x = train['budget'], y = revenue)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the plot shows a slight positive correlation between budget and revenue. Probably only for budgets > 100 million, there is a more linear relation to revenue. But for smaller budgets, the relation is not that strong. \n\nAnyway, the budget is a continuous numerical variable that describes precisely well, the budget of the movie. So no additional processing is needed here. "},{"metadata":{},"cell_type":"markdown","source":"**Release Date**\n\nNotice we have the feature 'release date' in the format MM/DD/YY. To extract more information out of this feature and create more insight into predicting movie revenue, we can split this up into release year, release month and release day. That we can try finding trends between revenue vs release year, month or day of the month. So hence we split up this feature to create three new features. \nAlso note that, the year is in the form YY so to convert to proper form, we add 1900 when YY is less than 19 since we know there is no movie in our database that was released before 1919 so any movie that has values between 0 to 19 has to have been released between 2000 and 2019 and not before. And for YY > 19, we add 1900 as a result. "},{"metadata":{"trusted":true},"cell_type":"code","source":"combined[['release_month','release_day','release_year']] = combined['release_date'].str.split('/', expand = True).replace(np.nan,-1).astype(int)\n\ncombined.loc[ (combined['release_year'] > 19), \"release_year\"] += 1900\ncombined.loc[ (combined['release_year'] <= 19), \"release_year\"] += 2000\ncombined[['release_day','release_month','release_year']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also make use of pd.to_datetime to convert our 'release date' column to standard pandas DateTime format so we can extract further information from this. Knowing what exact date it was, we can then create two new columns telling us what day of the week the movie released and what quarter it was. So let's do that then. \n\nNote - In release_dayofweek, the values go from 0-6 where 0 is Monday to 6 is Sunday. "},{"metadata":{"trusted":true},"cell_type":"code","source":"releaseData = pd.to_datetime(combined['release_date'])\ncombined['release_dayofweek'] = releaseData.dt.dayofweek\ncombined['release_quarter'] = releaseData.dt.quarter\ncombined.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There we have it. 5 useful extra columns from the release date feature. Now that we have extracted this information, we can drop the original release date feature. "},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.drop(columns = ['release_date'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the original feature is not there anymore. "},{"metadata":{},"cell_type":"markdown","source":"With our new-found features, we have the ability to extract some interesting insights and trends with the distribution of movies released based on release time. Let's first visualize the number of movies released per year. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,10))\nsns.countplot(combined['release_year'][:3000])\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, the basic trend is that the number of released movies increases year by year. The slump at 2017 can probably be attributed to incomplete records of movives in the entire year of 2017.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,12))\nsns.countplot(combined['release_month'][:3000])\nmonths = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\nloc, labels = plt.xticks()\nplt.xticks(loc, months, fontsize = 10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of movies released per month is quite equally spread out with a peak in the month of September. I don't know if that's representative of films in general or if it's just our data since our data isn't exactly sizeable enough. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,12))\nsns.countplot(combined['release_day'][:3000])\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, the most movies are released on the 1st of the month according to our data. Again, I don't know if that's representative of films in general. But otherwise, the numbers are pretty evenly distributed. The low number on 31st can be attributed to some months not having a 31st date. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,12))\nsns.countplot(combined['release_dayofweek'][:3000])\nloc, labels = plt.xticks()\nlabels = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']\nplt.xticks(loc, labels, fontsize = 10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, the vast majority of movies release on a Friday. Methods like these can be used to self-verify some processes in the way that seeing this data, we verified our release_dayofweek extraction. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,12))\nsns.countplot(combined['release_quarter'][:3000])\nloc, labels = plt.xticks()\nlabels = ['Q1','Q2','Q3','Q4']\nplt.xticks(loc, labels, fontsize=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pretty evenly spread across quarters with a slightly larger number of movies being released in the second half of the year. "},{"metadata":{},"cell_type":"markdown","source":"I now want to visualize the relation between our target variable - revenue and our newly created features. For this, I create a temporary df 'temp' where I put revenue back in order to visualize the below relationships. "},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = pd.concat([combined[:3000],revenue],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's visualize the mean movie revenue by year. "},{"metadata":{"trusted":true},"cell_type":"code","source":"temp['meanRevenueByYear'] = temp.groupby('release_year')['revenue'].aggregate('mean')\ntemp['meanRevenueByYear'].plot(figsize=(15,10),color=\"g\")\nplt.xlim(1920,2018)\nplt.xlabel(\"Release Year\")\nplt.ylabel(\"Revenue\")\nplt.title(\"Movie Mean Revenue By Year\",fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this visualization, it's quite interesting that film revenues peaked in the late 1970s and then quickly dipped after that only reaching similar numbers in the early 2000s. It's also worth noting that our mean revenue is oscillating year-by-year and that there is no steady year-by-year increase in revenue. "},{"metadata":{"trusted":true},"cell_type":"code","source":"temp['meanRevenueByMonth'] = temp.groupby(\"release_month\")[\"revenue\"].aggregate('mean')\ntemp['meanRevenueByMonth'].plot(figsize=(15,10),color=\"g\")\nplt.xlabel(\"Release Month\")\nplt.ylabel(\"Revenue\")\nplt.xlim(1,12)\nplt.title(\"Movie Mean Revenue Release Month\",fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is probably one of my favourite visualizations where we self confirm some things that we know to be true. The biggest movies mostly release in the summer or the end of the year! "},{"metadata":{"trusted":true},"cell_type":"code","source":"temp['meanRevenueByDayOfWeek'] = temp.groupby(\"release_dayofweek\")[\"revenue\"].aggregate('mean')\ntemp['meanRevenueByDayOfWeek'].plot(figsize=(15,10),color=\"g\")\nplt.xlabel(\"Day of Week\")\nplt.ylabel(\"Revenue\")\nplt.xlim(0,7)\nplt.title(\"Movie Mean Revenue by Day of Week\",fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Very surprisingly, the mean revenue is largest for movies released on Wednesdays and Thursdays (3 and 4). This might even be attributed to the small number of movies released on these days and a disproportionate sample of movies in our data as a result. But I do wonder if this finding holds true for movies in general too. "},{"metadata":{"trusted":true},"cell_type":"code","source":"temp['meanRevenueByQuarter'] = temp.groupby(\"release_quarter\")[\"revenue\"].aggregate('mean')\ntemp['meanRevenueByQuarter'].plot(figsize=(15,10),color=\"g\")\nplt.xlabel(\"Quarter\")\nplt.ylabel(\"Revenue\")\nplt.xlim(1,4)\nplt.title(\"Movie Mean Revenue by Quarter\",fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, it looks like the highest earning movies get released in the 2nd quarter (April-June). "},{"metadata":{},"cell_type":"markdown","source":"**Genres**\n\nLet's now look at another very important variable that may also arguably have the largest effect on box-office revenue - the genre. To think of going to the cinema to watch a movie, one would probably want to know what kind of genre the movie falls under and that plays a massive role in knowing how well a movie is going to do. Be it drama, action, fantasy, thriller, comedy - each genre creates an image and an expectation in the audience for what to expect. \n\nIn our training set, we have the 'genres' column in JSON format where from a Python perspective, you can view it as a list of dictionaries where each dictionary contains two key-value pairs: one for 'id' and one for 'name'. Expectedly, some movies might have only one genre defined whereas others have multiple ones. What we'll do in the below code is extract the genres from each entry and finally create columns corresponding to each genre so as to convert this categorical information to more analyzable quantitative data. "},{"metadata":{},"cell_type":"markdown","source":"Below, we find out which movie IDs have missing values in the 'genre' column. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_genres_na = combined['genres'].isna()\nna_id = []\nfor i in combined['id'][df_genres_na]:\n    na_id.append(i)\nprint('The list of Movie IDs that have missing genres are',na_id)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While working with the data, I realized that while reading data from the CSV file into the pandas DF, my 'genres' column has been converted into a string where ideally it should have been interpreted as a list of dictionaries. To change that, I use the literal_eval function to interpret the 'genre' entries as Python lists of dictionaries. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from ast import literal_eval\nfor i in range(len(combined['genres'])):\n    if i+1 not in na_id:\n        combined['genres'][i] = literal_eval(combined['genres'][i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out = {}\nl = []\nfor i in range(len(combined['genres'][:])):\n    if i+1 not in na_id:\n        for d in combined['genres'][i]:\n            for key,val in d.items():\n                if key == 'name':\n                    l.append(val)\n        out[i+1] = l\n        l = []\n        \nfor i in na_id:\n    out[i] = ['Drama']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the below code, we use multiple string operations to convert our JSON data into the most usable form. I have tried using basic Python string functions to make the code more readable and more intuitive to understand the data manipulation at play. The aim was to convert each entry to one that only contains their genres seperated by a space so that we can directly use the get_dummies function to create a one-hot encoding column for each unique genre. \n\nBelow, I also create a word cloud that gives us a real good sense of how common each genre is. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\ncombined_genres = pd.Series(data = out, index = out.keys())\ncombined_genres.sort_index(inplace = True)\n\nfor i in range(1,len(combined_genres)+1):\n    combined_genres[i] = str(combined_genres[i])[1:-1]\n    combined_genres[i] = combined_genres[i].replace(' ','')\n    combined_genres[i] = combined_genres[i].replace('\\'','')\n    combined_genres[i] = combined_genres[i].replace(',',' ')\n    \nword_cld = combined_genres[:3000]\ntext = ''\nfor i in range(1,len(word_cld)+1):\n    word_cld[i] = word_cld[i].replace(',',' ')\n    text = text + ' ' + word_cld[i]\n        \nwc = WordCloud(background_color = 'white', collocations = False, height  = 800, width = 1600).generate(text)\nplt.figure(figsize = (15,15))\nplt.title('Top Movie Genres')\nplt.imshow(wc, interpolation = 'bilinear')\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see in the Word Cloud above, it looks like Drama and Comedy followed by Thriller and Action are the most common genres in our data. This is pretty representative of films in general too and so we can say our data set samples overall film genre data considerably well. "},{"metadata":{},"cell_type":"markdown","source":"Here is where we use combined_genres to create our one-hot encoding columns correspdonding to each unique genre. This needs to be done to convert our categorical information into numeric so we can easily train our data into the model. As you can see below, new columns are created corresponding to the genre. For each row, there will be a 1 in a genre column if the genre exists for that movie and 0 otherwise. "},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_genres = combined_genres.reset_index(drop = True)\ncombined_genres_df = combined_genres.str.get_dummies(sep = ' ')\ncombined = pd.concat([combined,combined_genres_df], axis = 1)\ncombined.drop(['genres'], axis = 1, inplace = True)\ncombined.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the new binary valued genre columns correspdonding to each unique genre have been added to the combined dataframe and we do not need the original genres feature anymore.  "},{"metadata":{},"cell_type":"markdown","source":"**Production Companies**\n\nLet's move onto our next feature - Production companies. In my opinion, this probably ranks among the top 5 features to predict the movie revenue atleast from an intuitive sense. Seldom do you see movies that make big bucks that are not backed by a major production house. Movies made by major production studios often have considerably large budgets as well and are geared towards making large profits. \n\nAnother factor is that these movies have large marketing and advertising budgets which come from the production studio and these in turn lead to more people coming to the theatre while small studios or indie movies don't have that kind of luxury. \n\nLet's follow a similar process as we did for extracting genres out of the column. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_prod_comp_na = combined['production_companies'].isna()\nna_id_prod_comp = []\nfor i in combined['id'][df_prod_comp_na]:\n    na_id_prod_comp.append(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(combined['production_companies'])):\n    if i+1 not in na_id_prod_comp:\n        combined['production_companies'][i] = literal_eval(combined['production_companies'][i])\n\ntrain['production_companies'] = combined.loc[:3000,'production_companies']        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that I have duplicated the converted list items in 'combined' - production companies to 'train' - production companies.   "},{"metadata":{},"cell_type":"markdown","source":"Find all the rows where 'production companies' column is not NA. "},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train['production_companies'].notna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice how I'm using the train dataframe here. This is because I cannot take information that is being used to create a feature from the test set. We should be only using the training set for processing and feature engineering purposes and while only creating the new feature, it should be created on both sides. For the same reason, I will be using the train dataset instead of combined below. Since I don't want to use any information from the test set to create new features."},{"metadata":{},"cell_type":"markdown","source":"Here, we are extracting the production company names from the dictionary by taking value of key = 'name'.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_companies = list(train['production_companies'][X].apply(lambda x: [i['name'] for i in x] if x != \"\" else []).values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the 15 most common production companies in our training dataset! "},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\ntop_prod_comp = Counter([i for j in list_of_companies for i in j]).most_common(15)\ntop_prod_comp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nAs we can see, the usual suspects are in the list. The big guns, the premiere movie making studios of Hollywood. The Disneys, The Foxes, the WBs, etc. These studios are mostly behind the biggest blockbusters ever made in cinema. It is worth noting again that this data is representative of films in general. \n\nOther than getting some findings about the production company data, the idea behind taking the 15 most common production companies is so we can create 15 columns corresponding to each company. In the case of genres, we could do this for all the genres present as there was a small number of them. Here, the number of unique production studios were approximately 7000 which makes it nonsensical to create a one-hot column for each. So we select the 15 most common studios and create a column for each. "},{"metadata":{"trusted":true},"cell_type":"code","source":"X = combined['production_companies'].notna()\ncombined['all_production_companies'] = combined['production_companies'][X].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nfor i in range(len(combined['all_production_companies'])):\n    if i+1 in na_id_prod_comp:\n        combined['all_production_companies'][i] = 'Missing' \n    \ntop_companies = [m[0] for m in Counter([i for j in list_of_companies for i in j]).most_common(15)]\nfor g in top_companies:\n    combined['Prod_Comp_' + g] = combined['all_production_companies'].apply(lambda x: 1 if g in x else 0)\n\ncombined","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To make sure the right columns are in my dataframe, I usually take a quick glance at the columns list to check if my step-by-step progress is as intended. "},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So as we can see, we have 15 new columns with 'Prod_Comp_' prefixed before each Production Company name. We won't be needing the 'all_production_companies' column and the original JSON 'production_companies' column now so we drop it from our combined dataframe.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.drop(columns = ['production_companies','all_production_companies'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Production Countries**\n\nMoving onto our next feature - Production countries. This is also in JSON format and hence we will follow a similar pattern of extracting the data like genre and production company. I'm not sure how the correlation between production country and revenue would be. But I expect a large number of production countries to correspond to USA. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_prod_cntry_na = combined['production_countries'].isna()\nna_id_prod_cntry = []\nfor i in combined['id'][df_prod_cntry_na]:\n    na_id_prod_cntry.append(i)\n\nfor i in range(len(combined['production_countries'])):\n    if i+1 not in na_id_prod_cntry:\n        combined['production_countries'][i] = literal_eval(combined['production_countries'][i])\n\ntrain['production_countries'] = combined.loc[:3000,'production_countries']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train['production_countries'].notna()\nlist_of_countries = list(train['production_countries'][X].apply(lambda x: [i['name'] for i in x] if x != \"\" else []).values)\nCounter([i for j in list_of_countries for i in j]).most_common(25)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected! Around 75% of the movies in the data-set have USA as their production country which could be attributed to the large number of Hollywood movies in the data. The number of films having other production countries are very small and what would make sense would be to create a new feature only telling us whether the movie was shot in USA and not having a seperate column for each production country. So we have essentially divided the data into US and non-US in this aspect after seeing the disproportionately high number of movies shot in USA. "},{"metadata":{"trusted":true},"cell_type":"code","source":"X = combined['production_countries'].notna()\ncombined['all_countries'] = combined['production_countries'][X].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_countries = [m[0] for m in Counter([i for j in list_of_countries for i in j]).most_common(1)]\nfor g in top_countries:\n    combined['production_country_' + g] = combined['all_countries'][X].apply(lambda x: 1 if g in x else 0)\ncombined.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, a new column indicating movie production country in USA or not has been added to our dataframe. Since we have this, we won't be needing the 'all_countries' column and the original 'production_countries' column. "},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.drop(columns = ['all_countries','production_countries'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Cast**\n\nWhat's the first thing you do when you hear of a movie? Log onto Google and see who's in it ofcourse! That's where the cast comes in. The faces you see on the big screen, playing characters and immersing the audience into their world with their immense acting prowess. \n\nSome of these stars have the power to make the movie millions of dollars on their own. These stars have a fanbase that spans across the world and have acquired a reputation through decades of work in the industry. The general feeling is that one is less likelier to go watch a movie when the cast is unrecognizable. Sometimes, it doesn't even matter what the movie is about - as long as your favourite actor in it. Am I right? Therefore, I feel the cast has a vital role in predicting movie success.    \n\nThe 'cast' column is in JSON format too and we use a similar process as before in extracting relevant data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cast_na = combined['cast'].isna()\nna_id_cast = []\nfor i in combined['id'][df_cast_na]:\n    na_id_cast.append(i)\n\nfor i in range(len(combined['cast'])):\n    if i+1 not in na_id_cast:\n        combined['cast'][i] = literal_eval(combined['cast'][i])\n\ntrain['cast'] = combined.loc[:3000,'cast']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train['cast'].notna()\nprint('The number of cast members in films vs count')\ntrain['cast'][X].apply(lambda x: len(x) if x != {} else 0).value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like the most common number of cast members are 15 followed by 16 which seems like a fair number for a movie cast. Looking at the data, we can say most casts have numbers of around 9-18 cast members.  "},{"metadata":{},"cell_type":"markdown","source":"Let's dive a little deeper into our 'cast' column which not only has cast names but genders and characters too. Given our data, I'm a little curious to find out who the most common cast actors in our dataset are. Again, note that I want to see the most common actors in our training set only and not our combined (train + test) set. Since this information will be used to create new features as we've done earlier, I can't use any information from our test set. \n\nBelow is the code and the output for the 20 most common actors in our training data!"},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_cast_names = list(train['cast'][X].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_cast_names for i in j]).most_common(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that Samuel Jackson and Robert de Niro - two stallwarts of the Hollywood film industry share first place with 30 movies each. They are followed by other popular actors who have decades of films under their belt and rightly make it to our list. "},{"metadata":{},"cell_type":"markdown","source":"So what do we do with this data? It's nice getting an idea of the number of cast members for a movie and seeing the most popular actors in our data but what next? How do we potentially create or modify features to help predict movie revenue? \n\nI think creating seperate columns corresponding to our most popular actors would be a bad idea because even our most popular actors come in only 30 movies which is 1% of our data. Plus having those many extra columns with not a lot of information would be computationally inefficient. I would rather have a column that tells me if the film contains atleast one popular actor. Here, I create a list containing the 50 most popular cast actors and create a binary valued column 'cast_top_50' telling me if a film contains any of these 50 actors or not.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"top_cast_names = [m[0] for m in Counter([i for j in list_of_cast_names for i in j]).most_common(50)]\n\ntrain_cast = pd.DataFrame()\nfor g in top_cast_names:\n     train_cast['cast_' + g] = combined['cast'].apply(lambda x: 1 if g in str(x) else 0)\n\ntrain_cast['cast_sum'] = train_cast.sum(axis=1)\ntrain_cast['cast_top_50'] = train_cast['cast_sum'] > 0\ntrain_cast['cast_top_50'] = train_cast['cast_top_50'].astype(int)\ntrain_cast","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, this temporary dataframe train_cast has 52 columns - 50 actors, 1 sum of their appeearances and our intended column cast_top_50. Now we want to use only the cast_top_50 column and merge it our with original dataframe. "},{"metadata":{"trusted":true},"cell_type":"code","source":"combined = pd.concat([combined,train_cast['cast_top_50']], axis=1)\ncombined.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There we have it. The 'cast_top_50' column which tells us if the movie contains atleast one of the top 50 most popular actors from our training data. "},{"metadata":{},"cell_type":"markdown","source":"Our data for the 'cast' column contains gender of the cast member too so let's try extracting information from here as well. "},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_cast_genders = list(train['cast'][X].apply(lambda x: [i['gender'] for i in x] if x != {} else []).values)\ncast_gender_count = Counter([i for j in list_of_cast_genders for i in j]).most_common()\nprint(cast_gender_count)\nx = [a[0] for a in cast_gender_count]\ny = [a[1] for a in cast_gender_count]\nplt.figure(figsize = (8,8))\nplt.bar(x, height = y, width = 0.4, tick_label = ['Male','Unspecified','Female'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see in our bar graph, the number of male actors is around twice the number of female actors. A disadvantage of this data is that a large number of gender values are unspecified and this gives us no useful information. \n\nNext, let's move onto creating new features with the cast genders. Let's create 3 new columns that contain the sum of female (1) and male (2) cast members for a movie. I think a good feature would also be the gender ratio of the cast which is the number of female cast members to the number of male cast members. "},{"metadata":{"trusted":true},"cell_type":"code","source":"X = combined['cast'].notna()\ncombined['cast_genders_female'] = combined['cast'][X].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ncombined['cast_genders_male'] = combined['cast'][X].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\ncombined['cast_gender_ratio'] = combined['cast_genders_female']/combined['cast_genders_male']\ncombined.drop(columns = ['cast'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With our gender ratio feature, let's try seeing the relation between this and overall movie revenue. I create a scatter plot with gender ratio and revenue as our axes. Along with this, I implement linear regression to plot a linear model between these variables as seen by the red line in the figure. This would give us a good idea about the relation between gender ratio and revenue. "},{"metadata":{"trusted":true},"cell_type":"code","source":"x = combined['cast_gender_ratio'][:3000]\ny = revenue\nXY = pd.concat([x,y], axis = 1)\npd.options.mode.use_inf_as_na = True\nXY = XY.dropna()\n\nx = XY.iloc[:,0].to_numpy()\ny = XY.iloc[:,1]\nplt.figure(figsize = (15,10))\nplt.xlim(0,5)\nplt.scatter(x, y, s = 1) \n\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(x.reshape(-1,1),y)\nY_pred = model.predict(x.reshape(-1,1))\n\nplt.xlabel('Cast Gender Ratio [No. of females/No. of males]')\nplt.ylabel('Revenue')\nplt.title('Cast Gender Ratio vs Revenue')\nplt.plot(x,Y_pred, color='red', linewidth = 0.1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like the line has a slightly negative slope. That means, the lower the gender ratio or the lower the number of females to the number of males, the higher would be the movie revenue. Anyway, the line is slighly tilted and is almost horizontal to the x-axis so I don't presume this will have a massive effect on our revenue variable but extracting interesting insights like these during our EDA is essential to me to get a clear picture of the data and its features.   "},{"metadata":{},"cell_type":"markdown","source":"**Crew**\n\nNext up in line - the crew! I feel the crew other than probably the director, the writer and the music director, are not factors that majority of the audience pay attention to. These are the guys behind the scenes toiling to make the movie work from photography to cinematography to screenplay. The audience doesn't see who these crew members are and then decide whether to go to the movie or not. But what I can infer is that most of the good movies have a solid crew backing them and are essential to the success of a movie. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_crew_na = combined['crew'].isna()\nna_id_crew = []\nfor i in combined['id'][df_crew_na]:\n    na_id_crew.append(i)\n\nfor i in range(len(combined['crew'])):\n    if i+1 not in na_id_crew:\n        combined['crew'][i] = literal_eval(combined['crew'][i])\n\ntrain['crew'] = combined.loc[:3000, 'crew']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train['crew'].notna()\ntrain['crew'][X].apply(lambda x: len(x) if x != {} else 0).value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a surprising result. It seems that a crew of only 2 members is the most common in our data. Although the next common is 11 members, 3 members come in 3rd place. I think this has to do with the data preparation more than anything else. Since I'm sure a whole movie can't do with just 2 crew members. So yes, this should mostly be attributed to the data recording.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_crew_genders = list(train['crew'][X].apply(lambda x: [i['gender'] for i in x] if x != {} else []).values)\ncrew_gender_count = Counter([i for j in list_of_crew_genders for i in j]).most_common()\nprint(crew_gender_count)\nx = [a[0] for a in crew_gender_count]\ny = [a[1] for a in crew_gender_count]\nplt.figure(figsize = (6,6))\nplt.bar(x, height = y, width = 0.4, tick_label = ['Unspecified','Male','Female'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We explore genders in the crew too. A huge 41,787 genders are unspecified in our data which sadly gives us no gender-related information. We see that the gender ratio of the crew is even more skewed compared to the cast where there are 4 male crew members for every female crew members. \n\nLet's now create the 2 gender columns like we did for cast as well as create a crew gender ratio column.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"X = combined['crew'].notna()\ncombined['crew_genders_female'] = combined['crew'][X].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ncombined['crew_genders_male'] = combined['crew'][X].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\ncombined['crew_gender_ratio'] = combined['crew_genders_female']/combined['crew_genders_male']\ncombined.drop(['crew'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = combined['crew_gender_ratio'][:3000]\ny = revenue\nXY = pd.concat([x,y], axis = 1)\npd.options.mode.use_inf_as_na = True\nXY = XY.dropna()\nx = XY.iloc[:,0].to_numpy()\ny = XY.iloc[:,1]\nplt.figure(figsize = (15,12))\nplt.xlim(0,4)\nplt.scatter(x, y, s = 1) \n\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(x.reshape(-1,1),y)\nY_pred = model.predict(x.reshape(-1,1))\n\nplt.xlabel('Crew Gender Ratio [No. of females/No. of males]')\nplt.ylabel('Revenue')\nplt.title('Crew Gender Ratio vs Revenue')\nplt.plot(x,Y_pred, color='red', linewidth = 0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's rather nice seeing that the crew gender ratio and the revenue have a positive relation albeit slightly. This was contrary to the relation between cast gender ratio and revenue. So looks like having an equitable gender ratio in the crew might earn you more money! "},{"metadata":{},"cell_type":"markdown","source":"**Language**\n\nBelow, we move on to a movie's language! This is one feature that geographically divides the entire set to each local film industry. After plotting the distribution however, we can see that an overwhelming majority of greater than 2500 movies which is more than 83% of our data belongs to English language movies. \n\nSeeing these numbers, I decided to move ahead by creating a new feature which indicates if the movie is English or not. Instead of dealing with seperate languages which are negligible by themselves in number, we can combine the rest into non-English and get a decent split of approx. 2500 - 500. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,10))\nsns.countplot(train['original_language'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined['Eng_Language'] = 0 \ncombined.loc[combined['original_language'] == \"en\", \"Eng_Language\"] = 1\ncombined.drop(columns = ['original_language'], axis = 1, inplace = True)\n\nXY = pd.concat([combined['Eng_Language'][:3000], revenue], axis = 1)\n\nsns.catplot(x = 'Eng_Language', y = 'revenue', data = XY);\nplt.title('Revenue vs Movie Language')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see in the figure above, English language movies tend to earn significantly more than non-English language movies worldwide. But because of the very low number of non-English language movies, this may not give a very accurate picture. However for our purposes, this can be a good feature. From what we can infer from the plot, if a movie is English, it has a relatively higher likelihood of earning more. "},{"metadata":{},"cell_type":"markdown","source":"**Spoken Languages**\n\nThese are the languages that have been used in the movie. For example, bilingual movies where the actors can converse in both French and English throughout the film. \n\nI think a useful feature we could create is if the movie is multilingual or not and whether that has a relation with film revenue. Could it be that movies with more than one language appeal to a wider audience and hence have larger revenue? Let's find out. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sl_na = combined['spoken_languages'].isna()\nna_id_sl = []\nfor i in combined['id'][df_sl_na]:\n    na_id_sl.append(i)\n\nfor i in range(len(combined['spoken_languages'])):\n    if i+1 not in na_id_sl:\n        combined['spoken_languages'][i] = literal_eval(combined['spoken_languages'][i])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = combined['spoken_languages'].notna()\ncombined['more_than_1_language'] = combined['spoken_languages'][X].apply(lambda x: len(x) > 1 if x != {} else 0)\ncombined['more_than_1_language'] = combined['more_than_1_language'][X].astype(int)\ncombined.drop(columns = ['spoken_languages'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's see if multi-lingual movies do better than movies with one spoken language!"},{"metadata":{"trusted":true},"cell_type":"code","source":"XY = pd.concat([combined['more_than_1_language'][:3000], revenue], axis = 1)\nsns.catplot(x=\"more_than_1_language\", y=\"revenue\", data=XY)\nplt.title('Revenue vs Movies more than 1 spoken language')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Turns out that the relation with revenue is not that different on both sides - movies having 1 spoken language and movies having more than 1 spoken language. \n\nSince our data is dominated by English speaking movies, I feel distinctions on the basis of language won't drastically make a difference in revenue prediction. "},{"metadata":{},"cell_type":"markdown","source":"**Homepage**\n\nLet's now look at the 'homepage' column. Initially, I couldn't think of anything useful I could do with this feature until I saw the number of missing values in this column. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The number of missing values for homepage are',sum(train['homepage'].isna()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What I have learnt over different projects is that the existence of a feature can be a great feature in itself! \n\nWith over 2000 missing homepages, less than 1000 movies only have homepages. And this is a good split to have. So let's see if there's any relation between the existance of a homepage and movie revenue. "},{"metadata":{"trusted":true},"cell_type":"code","source":"combined['homepage_exists'] = 1\ncombined.loc[(combined['homepage'].isna()) ,\"homepage_exists\"] = 0\ncombined.drop(columns = ['homepage'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XY = pd.concat([combined['homepage_exists'][:3000], revenue], axis = 1)\nsns.catplot(x=\"homepage_exists\", y=\"revenue\", data=XY)\nplt.title('Revenue vs Homepage existence')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great! As we can see, there is a clear indication that films that have a homepage perform significantly better than films don't. The existence of a website can maybe be linked to higher budget films or bigger production companies but what's important here is that we have created a new feature that is strongly linked to box-office revenue. \n\nMoreover, this feature was created based on the existence of the feature itself. The website by itself was giving us no useful information but its existence is what matters here. "},{"metadata":{},"cell_type":"markdown","source":"**Belongs to collection**\n\nLet's now extend a similar analogy to the 'belongs_to_collection' part. The data in itself isn't very useful as it only tells you the name of the franchise or collection the movie belongs to. But what is important here is IF the movie belongs to a collection or not. What if movies already part of a franchise earn more because more people know about them?  \n\nLet's find out! "},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The number of missing values in belongs_to_collection feature are',sum(train['belongs_to_collection'].isna()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is almost 80% of the data but it is logical considering only a minority of movies belong to a collection while most of them are independent ones. \n\nLet's make a column that tells us whether a movie belongs to a collection or not. "},{"metadata":{"trusted":true},"cell_type":"code","source":"combined['part_of_collection'] = 1\ncombined.loc[(combined['belongs_to_collection'].isna()) ,\"part_of_collection\"] = 0\ncombined.drop(columns = ['belongs_to_collection'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Like we did before, let's examine the relation between belonging to a collection and movie revenue."},{"metadata":{"trusted":true},"cell_type":"code","source":"XY = pd.concat([combined['part_of_collection'][:3000], revenue], axis = 1)\nsns.catplot(x=\"part_of_collection\", y=\"revenue\", data=XY)\nplt.title('Revenue vs Belonging to collection')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we expected, movies belonging to a collection slightly outperform in revenue. This may be attributed to many factors including better awareness among audience and building up fanbases.  "},{"metadata":{},"cell_type":"markdown","source":"**Tagline**\n\nSince this worked well, we can try extrapolating this understanding to other features like the tagline of the movie. Let's see if we can use its existence as a feature by seeing the number of missing values. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The number of missing values for tagline are',sum(train['tagline'].isna()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's close to 20% of the data. So we can try seeing if there's a relation between revenue and tagline existence now! "},{"metadata":{"trusted":true},"cell_type":"code","source":"combined['tagline_exist'] = 1\ncombined.loc[combined['tagline'].isna() ,\"tagline_exist\"] = 0\ncombined.drop(columns = ['tagline'], axis = 1, inplace = True)\n\nXY = pd.concat([combined['tagline_exist'][:3000], revenue], axis = 1)\nsns.catplot(x=\"tagline_exist\", y=\"revenue\", data=XY)\nplt.title('Revenue vs Existence of a tagline')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And we see a similar trend here too! The existence of a tagline is indeed correlated to the movie revenue. So for a given movie, having a tagline increases the chances of it earning well at the box-office. "},{"metadata":{},"cell_type":"markdown","source":"**Title, Original Title, Keywords and Overview** \n\nBelow, I will look at the textual features in our data - Title, Original Title, Keywords and Overview, that don't have a lot of missing values. So this eliminates the possibility of using the absence of these features as features as we've done above. \n\nFor now, let's first get an idea of what the data looks like by using my favourite infographic for textual data - the wordcloud!"},{"metadata":{},"cell_type":"markdown","source":"**Title**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 12))\ntext = ' '.join(train['title'].values)\nwordcloud = WordCloud(max_font_size=None, background_color='white', width=1200, height=1000).generate(text)\nplt.imshow(wordcloud)\nplt.title('Top words in titles')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Original Title**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 12))\ntext = ' '.join(train['original_title'].values)\nwordcloud = WordCloud(max_font_size=None, background_color='white', width=1200, height=1000).generate(text)\nplt.imshow(wordcloud)\nplt.title('Top words in titles')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Keywords**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 12))\ntext = ' '.join(train['Keywords'][train['Keywords'].notna()].values)\nwordcloud = WordCloud(max_font_size=None, background_color='white', width=1200, height=1000).generate(text)\nplt.imshow(wordcloud)\nplt.title('Top words in Keywords')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Overview**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 12))\ntext = ' '.join(train['overview'][train['overview'].notna()].values)\nwordcloud = WordCloud(max_font_size=None, background_color='white', width=1200, height=1000).generate(text)\nplt.imshow(wordcloud)\nplt.title('Top words in overview')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'd like to quote 'a picture is worth a thousand words' here. From these wordclouds, we do get a good idea of the words, their occurences, common underlying themes and patterns in the movies. \n\nIn my honest opinion, I highly doubt there could be a tangible relation between the Title and Revenue. Note, the original title may be different from the title when the movie is non-English, otherwise they are the same. The title rarely tells you anything about the movie in itself and this is where it contrasts with Overview and Keywords. \n\nOverview and Keywords on the other hand do tell us what the movie is about. In a way, it is analogous to the genre of the movie where the overview and keywords list themes, buzzwords and characteristics of the movie. A very short summary of the movie if you may! \n\nIdeally, I would want to create a language model with these textual columns that could identify language patterns and semantics to help predict revenue. But I'm afraid our training data is nearly not enough to build a good model. At this point, it would be computationally inefficient to apply NLP on our training data and using the most common words as features won't do this justice too. \n\nSo as a later extension, I will try to work solely with these textual columns, gather a lot more data and use NLP techniques and processes to extract information from this data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.drop(columns = ['title','original_title','Keywords','overview'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Popularity**\n\nI'm not sure what the 'Popularity' variable is and moreover, how it was measured. It seems to be an indicator of well, how popular the movie is with audience. Since it needs no additional processing, let's see the correlation with revenue. "},{"metadata":{"trusted":true},"cell_type":"code","source":"x = combined['popularity'][:3000]\ny = revenue\nplt.figure(figsize=(16, 8))\nplt.scatter(x, y, s=2)\nplt.xlim(0,50)\nplt.ylim(0,0.4*(10**9))\nplt.xlabel('Popularity')\nplt.ylabel('Revenue')\nplt.title('Revenue vs popularity')\n\nfrom sklearn.linear_model import LinearRegression\nx = x.to_numpy()\nmodel = LinearRegression()\nmodel.fit(x.reshape(-1,1),y)\nY_pred = model.predict(x.reshape(-1,1))\n\nplt.plot(x,Y_pred, color='green', linewidth = 0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nice! Looks like the correlation with revenue is pretty positive. 'Popular' movies make the more money I assume! We could get a little more insight from this data if we knew how this popularity variable was measured in the first place. For now, it seems like a good feature to indicate revenue. "},{"metadata":{},"cell_type":"markdown","source":"**RUNTIME**\n\nLet's move to run-time now. I highly doubt the run time will have an effect on the revenue of a movie as most run times are distributed between 90 and 150 minutes. It's almost like a standard of cinema everywhere and I don't see how movie run-time will affect movie revenue in any way. Let's check the distribution of runtime in a histogram though. I'm expecting it to be a bell curve.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,10))\nplt.xlabel('Runtime')\nplt.ylabel('Frequency')\nplt.title('Distribution of runtimes')\nplt.hist(train['runtime'], bins = 50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, it is approximately a skewed bell curve with a peak at around 100 minutes (1hr 40 min) of runtime. "},{"metadata":{},"cell_type":"markdown","source":"**STATUS**\n\nAt first glance, I see most of the values in the 'status' column are 'Released'. Let's do a value count to see the distinct values in the column."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['status'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Right, so looks like all the movies except 4 have the same value and these 4 weren't released. \n\nAnyway, this is a very, very small number to consider and so this feature is pretty insignificant in predicting revenue which is why I'm dropping it. "},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.drop(columns = ['status'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**IMDB ID and Poster Path**\n\nThe IMDB ID is the ID of the movie as listed by IMDB. The poster path is a link to the poster of the film. \nBoth these features wouldn't help in predicting revenue so I'll drop them. "},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.drop(columns = ['imdb_id','poster_path'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point, we've gone through all 21 of our input features. But wait, there were 22, right? Since the ID column is of no use, we drop that too. "},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.drop(columns = ['id'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now see what our final combined dataframe looks like after all the creating and dropping features. Keep in mind that each feature should have strictly numerical data before being passed on to train our machine learning model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print('A full view of our columns are',list(combined.columns))\ncombined","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks good! These 56 features are our final list of features that will be used to train our regression model. "},{"metadata":{},"cell_type":"markdown","source":"Since we have our final list of features, at this point I like making a feature correlation heatmap to tell me how correlated my input features are with each other and moreover, how correlated they are with revenue. Below, I've taken a few select input features and the revenue from my training data and plotted this heatmap. "},{"metadata":{"trusted":true},"cell_type":"code","source":"XY = pd.concat([combined.iloc[:3000, :], revenue], axis = 1)\nfeatures_corr = XY[['budget','popularity','release_year','release_month','production_country_United States of America','part_of_collection','Eng_Language','homepage_exists','cast_gender_ratio','crew_gender_ratio','revenue']]\nf,ax = plt.subplots(figsize=(12, 12))\nsns.heatmap(features_corr.corr(), annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Along with seeing the correlation factors between different input features, pay special attention to the last line where we see the correlation between revenue and input features. Notice how budget and popularity have strong positive correlations which we know to be true. The higher the budget and popularity, the greater would be the revenue. \n\nFor release_month, notice the number is 0.02 which indicates almost no correlation with revenue. That in a way tells us that the release month would not have a substantial effect on the revenue. \n\nAlso notice how part_of_collection and homepage_exists have decently high numbers too. This is what we expected after visualizing the relationship earlier as well. \n\nLook at the relationship between crew_gender_ratio and cast_gender_ratio too. Like we saw, revenue has a negative relation with cast gender ratio while it has a positive relation with crew gender ratio which is evidenced here by positive and negative correlation factors. "},{"metadata":{},"cell_type":"markdown","source":"**Creating our training and testing data sets** \n\nOur combined dataframe has 7398 rows and 56 columns. Of these, we will extract the first 3000 rows (which was the original size of our training data) to train our model and the remaining 4398 rows which will be used to test our regression model and the output of which forms our final submission. To train the model, we also need to pass our output values which is the 'revenue' column from our initial training data of 3000 values. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = combined.iloc[:3000,:]\ntest = combined.iloc[3000:,:]\n\nprint('The shape of our training data is',train_X.shape)\nprint('The shape of our testing data is',test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use the natural log of our revenue values as our training output for which we will convert back to the original scale on creating our submission file. This is done so as not to overweight the blockbuster revenue movies earning well in excess of a billion dollars. (Hint: Marvel!)"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = np.log1p(revenue)\ny","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As is the norm with training machine learning models, I will use k-fold cross validation with k = 10. This value is quite standard in machine learning practise. \n\nIn essence, this will now create 10 different slabs of our training set each of which will be used as the testing set once and the other 9 slabs will be used to train our model. This gives us a much better sense of how our model really performs compared to avoiding k-fold cross validation. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n\nrandom_seed = 8\nk = 10\nfold = list(KFold(k, shuffle = True, random_state = random_seed).split(train_X))\nnp.random.seed(random_seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After passing our training data through a 10-fold, we move onto our machine learning algorithms. Gradient boosting decision tree ensemble methods continue to be state of the art in regression/classification ESPECIALLY with the size of our dataset and that's its structed/tabular. With bigger data, I naturally prefer to incorporate deep learning methods but that is not the case here. "},{"metadata":{},"cell_type":"markdown","source":"**XGBoost**\n\nXGBoost is a top-notch gradient boosting method because of it's parallelization, regularization methods, built in cross validation capability and ability to internally handle missing values. Below I just create a function feeding in my training and testing data. The parameter values I have used are the standard for XGBoost models. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\ndef xgb_model(trn_x, trn_y, val_x, val_y, test, verbose) :\n    \n    params = {'objective': 'reg:linear', \n              'eta': 0.01, \n              'max_depth': 6, \n              'subsample': 0.6, \n              'colsample_bytree': 0.7,  \n              'eval_metric': 'rmse', \n              'seed': random_seed, \n              'silent': True,\n    }\n    \n    record = dict()\n    model = xgb.train(params\n                      , xgb.DMatrix(trn_x, trn_y)\n                      , 100000\n                      , [(xgb.DMatrix(trn_x, trn_y), 'train'), (xgb.DMatrix(val_x, val_y), 'valid')]\n                      , verbose_eval=verbose\n                      , early_stopping_rounds=500\n                      , callbacks = [xgb.callback.record_evaluation(record)])\n    best_idx = np.argmin(np.array(record['valid']['rmse']))\n\n    val_pred = model.predict(xgb.DMatrix(val_x), ntree_limit=model.best_ntree_limit)\n    test_pred = model.predict(xgb.DMatrix(test), ntree_limit=model.best_ntree_limit)\n\n    return {'val':val_pred, 'test':test_pred, 'error':record['valid']['rmse'][best_idx], 'importance':[i for k, i in model.get_score().items()]}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Light GBM**\n\nThe Light GBM, also a sub-class of the gradient boosting decision trees classifiers, is known to perform extremely well on structured small-medium data such as this. \n\nRegarding which algorithm to use, the superiority of each algorithm is highly dependent on the data. This is why I'm going with my two confident choices and using both of these two algorithms and taking their average.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\n\ndef lgb_model(trn_x, trn_y, val_x, val_y, test, verbose) :\n\n    params = {'objective':'regression',\n         'num_leaves' : 30,\n         'min_data_in_leaf' : 20,\n         'max_depth' : 9,\n         'learning_rate': 0.004,\n         #'min_child_samples':100,\n         'feature_fraction':0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         'lambda_l1': 0.2,\n         \"bagging_seed\": random_seed,\n         \"metric\": 'rmse',\n         #'subsample':.8, \n          #'colsample_bytree':.9,\n         \"random_state\" : random_seed,\n         \"verbosity\": -1}\n\n    record = dict()\n    model = lgb.train(params\n                      , lgb.Dataset(trn_x, trn_y)\n                      , num_boost_round = 100000\n                      , valid_sets = [lgb.Dataset(val_x, val_y)]\n                      , verbose_eval = verbose\n                      , early_stopping_rounds = 500\n                      , callbacks = [lgb.record_evaluation(record)]\n                     )\n    best_idx = np.argmin(np.array(record['valid_0']['rmse']))\n\n    val_pred = model.predict(val_x, num_iteration = model.best_iteration)\n    test_pred = model.predict(test, num_iteration = model.best_iteration)\n    \n    return {'val':val_pred, 'test':test_pred, 'error':record['valid_0']['rmse'][best_idx], 'importance':model.feature_importance('gain')}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note - in both XGBoost and Light GBM, we have used the RMSE metric to calculate the minima. This makes it easier since the RMSE (Root Mean Square Error) is used as the metric in this competition to evaluate submissions. Here, it is calculated between our predicted revenue and the actual revenue. "},{"metadata":{},"cell_type":"markdown","source":"**Training and evaluating our model**\n\nWhat we do is essentially iterate through each of our 10 models created through 10-fold cross validation earlier. The training and testing sets are created each time based on the values in 'trn' and 'val' where 90% of the data is train and 10% is test for each iteration. We then pass our training data and testing data through our XGBoost and Light GBM model training functions. We give equal weights of 0.5 in our code to each model.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\n\nresult_dict = dict()\nval_pred = np.zeros(train.shape[0])\ntest_pred = np.zeros(test.shape[0])\nfinal_err = 0\nverbose = False\n\nfor i, (trn, val) in enumerate(fold) :\n    print('Fold number',i+1)\n    \n    trn_x = train_X.loc[trn, :]\n    trn_y = y[trn]\n    val_x = train_X.loc[val, :]\n    val_y = y[val]\n    \n    fold_val_pred = []\n    fold_test_pred = []\n    fold_err = []\n    \n    start = datetime.now()\n    result = xgb_model(trn_x, trn_y, val_x, val_y, test, verbose)\n    fold_val_pred.append(result['val']*0.5)\n    fold_test_pred.append(result['test']*0.5)\n    fold_err.append(result['error'])\n    print(\"XGBoost - \", \"{0:.5f}\".format(result['error']), '(' + str(int((datetime.now()-start).seconds/60)) + 'm)')\n    \n    start = datetime.now()\n    result = lgb_model(trn_x, trn_y, val_x, val_y, test, verbose)\n    fold_val_pred.append(result['val']*0.5)\n    fold_test_pred.append(result['test']*0.5)\n    fold_err.append(result['error'])\n    print(\"Light GBM - \", \"{0:.5f}\".format(result['error']), '(' + str(int((datetime.now()-start).seconds/60)) + 'm)')\n    \n    val_pred[val] += np.mean(np.array(fold_val_pred), axis = 0)\n    test_pred += np.mean(np.array(fold_test_pred), axis = 0) / k\n    final_err += (sum(fold_err) / len(fold_err)) / k\n    \n    print(\"---------------------------\")\n    print(\"The average error is \", \"{0:.5f}\".format(sum(fold_err) / len(fold_err)))\n    print(\"The blend error is \", \"{0:.5f}\".format(np.sqrt(np.mean((np.mean(np.array(fold_val_pred), axis = 0) - val_y)**2))))\n    \n    print('')\n    \nprint(\"FINAL average error = \", final_err)\nprint(\"FINAL blend error = \", np.sqrt(np.mean((val_pred - y)**2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There we go. We have our final RMSE figure which we hope will be close to what the leaderboard evaluates after submission. Since we have our set of 4398 predicted revenues, it's time to write it to a CSV file. Note that we multiply the figure since we used two models and we used np.expm1 to compensate for the natural log we took of revenue earlier. The file 'submission.csv' contains 2 columns - column 1 with the ID and column 2 with corresponding predicted revenue. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('./sample_submission.csv')\ndf_sub = pd.DataFrame()\ndf_sub['id'] = sub['id']\ndf_sub['revenue'] = np.expm1(test_pred*2)\ndf_sub.to_csv(\"../../working/submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The end **\n\nAnd that's it! I hope this walk-through was informative and gave some insight into the value and actionable insights a bunch of plain numbers can produce. I do hope to improve the model in the future but for now I take this experience forward with me to greater challenges.  \n\nIf anything, please feel free to get in touch with me at: fuzailm@ucla.edu. \nCheers. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}